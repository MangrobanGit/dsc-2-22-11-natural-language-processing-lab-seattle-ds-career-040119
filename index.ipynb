{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectorization - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we'll learn how tokenize and vectorize text documents, create an use a Bag of Words, and identify words unique to individual documents using TF-IDF Vectorization. \n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to: \n",
    "\n",
    "* Tokenize a corpus of words and identify the different choices to be made while parsing them\n",
    "* Use a Count Vectorization strategy to create a Bag of Words\n",
    "* Use TF-IDF Vectorization with multiple documents to identify words that are important/unique to certain documents\n",
    "\n",
    "## Let's get started!\n",
    "\n",
    "Run the cell below to import everything necessary for this lab.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.tokenize import word_tokenize\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Corpus\n",
    "\n",
    "In this lab, we'll be working with 20 different documents, each containing song lyrics from either Garth Brooks or Kendrick Lamar albums.  \n",
    "\n",
    "The songs are contained within the `data` subdirectory, contained within the same folder as this lab.  Each song is stored in a single file, with files ranging from `song1.txt` to `song20.txt`.  \n",
    "\n",
    "To make it easy to read in all of the documents, use a list comprehension to create a list containing the name of every single song file in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lyrics_helper.py  song12.txt  song16.txt  song1.txt   song4.txt  song8.txt\r\n",
      "lyrics_url.txt\t  song13.txt  song17.txt  song20.txt  song5.txt  song9.txt\r\n",
      "song10.txt\t  song14.txt  song18.txt  song2.txt   song6.txt\r\n",
      "song11.txt\t  song15.txt  song19.txt  song3.txt   song7.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['song1.txt', 'song2.txt', 'song3.txt', 'song4.txt', 'song5.txt', 'song6.txt', 'song7.txt', 'song8.txt', 'song9.txt', 'song10.txt', 'song11.txt', 'song12.txt', 'song13.txt', 'song14.txt', 'song15.txt', 'song16.txt', 'song17.txt', 'song18.txt', 'song19.txt', 'song20.txt']\n"
     ]
    }
   ],
   "source": [
    "filenames = ['song' + str(n+1) + '.txt' for n in range(20)]\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's import a single song to see what our text looks like so that we can make sure we clean and tokenize it correctly. \n",
    "\n",
    "In the cell below, read in and print out the lyrics from `song11.txt`.  Use vanilla python, no pandas needed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Kendrick Lamar:]\n",
      "\n",
      "Love, let's talk about love\n",
      "\n",
      "Is it anything and everything you hoped for?\n",
      "\n",
      "Or do the feeling haunt you?\n",
      "\n",
      "I know the feeling haunt you\n",
      "\n",
      "[SZA:]\n",
      "\n",
      "This may be the night that my dreams might let me know\n",
      "\n",
      "All the stars approach you, all the stars approach you, all the stars approach you\n",
      "\n",
      "This may be the night that my dreams might let me know\n",
      "\n",
      "All the stars are closer, all the stars are closer, all the stars are closer\n",
      "\n",
      "[Kendrick Lamar:]\n",
      "\n",
      "Tell me what you gon' do to me\n",
      "\n",
      "Confrontation ain't nothin' new to me\n",
      "\n",
      "You can bring a bullet, bring a sword, bring a morgue\n",
      "\n",
      "But you can't bring the truth to me\n",
      "\n",
      "Fuck you and all your expectations\n",
      "\n",
      "I don't even want your congratulations\n",
      "\n",
      "I recognize your false confidence\n",
      "\n",
      "And calculated promises all in your conversation\n",
      "\n",
      "I hate people that feel entitled\n",
      "\n",
      "Look at me crazy 'cause I ain't invite you\n",
      "\n",
      "Oh, you important?\n",
      "\n",
      "You the moral to the story? You endorsin'?\n",
      "\n",
      "Motherfucker, I don't even like you\n",
      "\n",
      "Corrupt a man's heart with a gift\n",
      "\n",
      "That's how you find out who you dealin' with\n",
      "\n",
      "A small percentage who I'm buildin' with\n",
      "\n",
      "I want the credit if I'm losin' or I'm winnin'\n",
      "\n",
      "On my momma, that's the realest shit\n",
      "\n",
      "Love, let's talk about love\n",
      "\n",
      "Is it anything and everything you hoped for?\n",
      "\n",
      "Or do the feeling haunt you?\n",
      "\n",
      "I know the feeling haunt you\n",
      "\n",
      "[SZA:]\n",
      "\n",
      "This may be the night that my dreams might let me know\n",
      "\n",
      "All the stars approach you, all the stars approach you, all the stars approach you\n",
      "\n",
      "This may be the night that my dreams might let me know\n",
      "\n",
      "All the stars are closer, all the stars are closer, all the stars are closer\n",
      "\n",
      "Skin covered in ego\n",
      "\n",
      "Get to talkin' like ya involved, like a rebound\n",
      "\n",
      "Got no end game, got no reason\n",
      "\n",
      "Got to stay down, it's the way that you making me feel\n",
      "\n",
      "Like nobody ever loved me like you do, you do\n",
      "\n",
      "You kinda feeling like you're tryna get away from me\n",
      "\n",
      "If you do, I won't move\n",
      "\n",
      "I ain't just cryin' for no reason\n",
      "\n",
      "I ain't just prayin' for no reason\n",
      "\n",
      "I give thanks for the days, for the hours\n",
      "\n",
      "And another way, another life breathin'\n",
      "\n",
      "I did it all 'cause it feel good\n",
      "\n",
      "I wouldn't do it at all if it feel bad\n",
      "\n",
      "Better live your life, we're runnin' out of time\n",
      "\n",
      "[Kendrick Lamar & SZA:]\n",
      "\n",
      "Love, let's talk about love\n",
      "\n",
      "Is it anything and everything you hoped for?\n",
      "\n",
      "Or do the feeling haunt you?\n",
      "\n",
      "I know the feeling haunt you\n",
      "\n",
      "[SZA:]\n",
      "\n",
      "This may be the night that my dreams might let me know\n",
      "\n",
      "All the stars approach you, all the stars approach you, all the stars approach you\n",
      "\n",
      "This may be the night that my dreams might let me know\n",
      "\n",
      "All the stars are closer, all the stars are closer, all the stars are closer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('./data/song11.txt') as f:\n",
    "    for line in f:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for file in filenames:\n",
    "    #print(file)\n",
    "    with open('./data/' + file) as f:\n",
    "        documents.append(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"My head is aching, and I'm late for work\\nThere's a girl in the kitchen, and she's wearing my shirt\\nMy buddies are home again, they're threatening to leave\\nYes that beer on my nightstand will be breakfast for me\\nCause that's how it goes with cowboys and friends\\nAs soon as it's over, it all starts again\\nThat's the way that it should be\\nCause that's the way that it's been\\nYeah the fun never ends, when the party begins\\nCowboys and friends\\nI've been working all morning, just busting my back\\nAnd it's all for a foreman who doesn't know jack\\nMy buddies keep talking, they say we're going out late\\nGuess that sleep that I'm wanting will just have to wait\\nCause that's how it goes with cowboys and friends\\nAs soon as it's over, it all starts again\\nThat's the way that it should be\\nCause that's the way that it's been\\nYeah the fun never ends, when the party begins\\nCowboys and friends\\nYeah the party begins where this two lane road ends\\nCowboys and friends\\n\",\n",
       " 'She worked the window at the Desert Sands\\nClyde was a dealer who was gifted with the sleight of hand\\nBang! Bang! Bang! Bang!\\nTheir petty thieving and conniving ways\\nWould never get \\'em to that beach house of their dreams someday\\nBang! Bang! Bang! Bang!\\nBut every afternoon at four that armored car rolled to the door (like before)\\nBang! Bang!\\nThe doors swing open and the pick-up begins\\nBang! Bang!\\nThe guard knocks, she lets him in\\nBang! Bang!\\nTwo bangs of cash hit the floor\\nThe guard asks her out, she turns him down and shows him the door (once more)\\nBang! Bang! Bang! Bang!\\nThey must have run through it a thousand times\\nBut something happened on the evening of their perfect crime\\nBang! Bang! Bang! Bang!\\nIt seems the set up and the switch all play\\nBut there\\'s a problem when it gets down to the get away\\nBang! Bang! Bang! Bang!\\nSo, to complete their master plan\\nThey\\'re gonna need a pick-up man (meet Jo-Ann)\\nBang! Bang! Bang!\\nIt\\'s four on Monday and the hustle begins\\nBang! Bang! Bang!\\nThe guard knocks, she lets him in\\nBang! Bang! Bang!\\nThe bags of cash hit the floor\\nClyde\\'s hiding out, he swaps \\'em out\\nThe guard\\'s out the door (they score)\\nBang! Bang! Bang!\\nBang! Bang! Bang!\\nTwo big bags of money and they\\'re both feeling light\\nShe sees the guard is stopping, she knows when something\\'s not right\\nWhen he turns to find he\\'s standing face to face now with her\\nShe says \"how \\'bout dinner Friday\\nAnd if you\\'re good I\\'ll let you have dessert\"\\nBang! Bang!\\nThe guard is happy and he\\'s gone in a flash\\nBang! Bang!\\nShe grabs Clyde and the cash\\nBang! Bang!\\nThey see Jo-Ann and jump in\\nThey were last seen heading West and never heard from again\\nThe end\\nBang! Bang!\\nAmen\\nBang! Bang!\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing our Data\n",
    "\n",
    "Before we can create a Bag of Words or vectorize each document, we need to clean it up and split each song into an array of individual words.  Computers are very particular about strings. If we tokenized our data in it's current state, we would run into the following problems:\n",
    "\n",
    "1. Counting things that aren't actually words.  In the example above, `\"[Kendrick]\"` is a note specifying who is speaking, not a lyric contained in the actual song, so it should be removed.  \n",
    "1. Punctuation and capitalization would mess up our word counts.  To the python interpreter, `love`, `Love`, `Love?`, and `Love\\n` are all unique words, and would all be counted separately.  We need to remove punctuation and capitalization, so that all words will be counted correctly. \n",
    "\n",
    "Consider the following sentences from the example above:\n",
    "\n",
    "`\"Love, let's talk about love\\n\", 'Is it anything and everything you hoped for?\\n'`\n",
    "\n",
    "After tokenization, this should look like:\n",
    "\n",
    "`['love', 'let's', 'talk', 'about', 'love', 'is', 'it', 'anything', 'and', 'everything', 'you', 'hoped', 'for']`\n",
    "\n",
    "Tokenization is pretty tedious if we handle it manually, and would probably make use of Regular Expressions, which is outside the scope of this lab.  In order to keep this lab moving, we'll use a library function to clean and tokenize our data so that we can move onto vectorization.  \n",
    "\n",
    "Tokenization is a required task for just about any Natural Language Processing (NLP) task, so great industry-standard tools exist to tokenize things for us, so that we can spend our time on more important tasks without getting bogged down hunting every special symbol or punctuation in a massive dataset. For this lab, we'll make use of the tokenizer in the amazing `nltk` library, which is short for _Natural Language Tool Kit_.\n",
    "\n",
    "**_NOTE:_** NLTK requires extra installation methods to be run the first time certain methods are used.  If `nltk` throws you an error about needing to install additional packages, follow the instructions in the error message to install the dependencies, and then rerun the cell.  \n",
    "\n",
    "Before we tokenize our songs, we'll do only a small manual bit of cleaning.  In the cell below, write a function that allows us to remove lines that have `['artist names']` in it, to ensure that our song files contain only lyrics that are actually in the song. For the lines that remain, make every word lowercase, remove newline characters `\\n`, and any of the following punctuation marks: `\",.'?!\"`\n",
    "\n",
    "Test the function on `test_song` to show that it has successfully removed `'[Kendrick Lamar:]'` and other instances of artist names from the song and returned it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_song = documents[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = ['Kendrick Lamar', 'Garth Brooks', 'SZA', \n",
    "           'Kendrick Lamar & SZA', 'Khalid', 'The Weeknd', \n",
    "           'Babes Wodumo', 'Zacari', 'Hykeem Carter (Kendrick Lamar)',\n",
    "           'Future & Kendrick Lamar', 'Jay Rock', 'James Blake',\n",
    "           'Future','Ab-Soul','Anderson .Paak', 'Jorja Smith',\n",
    "           'Jorja Smith (Kendrick Lamar)', 'Swae Lee',\n",
    "           'Swae Lee (Kendrick Lamar)']\n",
    "\n",
    "punctuations = \",.'?!()[]`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  miss me with that \n"
     ]
    }
   ],
   "source": [
    "def clean_song(song):\n",
    "    \"\"\"\n",
    "    Clean songs:\n",
    "    -Remove artists | remove newlines \n",
    "    \"\"\"\n",
    "    \n",
    "    # remove artist headers\n",
    "    for artist in artists:\n",
    "        song = song.replace('[' + artist  + ':]',' ')\n",
    "    \n",
    "    # remove newlines\n",
    "    song = song.replace('\\n',' ')\n",
    "    \n",
    "    # remove double backticks\n",
    "    song = song.replace('``',' ')\n",
    "    \n",
    "    # remove double quotes\n",
    "    song = song.replace(\"''\",' ')\n",
    "    \n",
    "    # every word lowercase\n",
    "    song = song.lower()\n",
    "    \n",
    "    # any of the following punctuation marks: \",.'?!\"\n",
    "    output = ''\n",
    "    for char in song:\n",
    "        if char in punctuations:\n",
    "            char = ' '\n",
    "        output += char\n",
    "    song = output\n",
    "    return song\n",
    "    \n",
    "song_without_brackets = clean_song(test_song)\n",
    "print(song_without_brackets[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now, write a function that takes in songs that have had their brackets removed, joins all of the lines into a single string, and then uses `tokenize()` on it to get a fully tokenized version of the song.  Test this funtion on `song_without_brackets` to ensure that the function works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/werlindo/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['miss',\n",
       " 'me',\n",
       " 'with',\n",
       " 'that',\n",
       " 'bullshit',\n",
       " 'bullshit',\n",
       " 'you',\n",
       " 'ain',\n",
       " 't',\n",
       " 'really']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(song):\n",
    "    \"\"\"\n",
    "    Input: string of song lyrics\n",
    "    Output: list of words(tokens)\n",
    "    \"\"\"\n",
    "    song = word_tokenize(song)\n",
    "    return song\n",
    "\n",
    "tokenized_test_song = tokenize(song_without_brackets)\n",
    "tokenized_test_song[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we know the ability to tokenize our songs, we can move onto Vectorization. \n",
    "\n",
    "### Count Vectorization\n",
    "\n",
    "Machine Learning algorithms don't understand strings.  However, they do understand math, which means they understand vectors and matrices.  By **_Vectorizing_** the text, we just convert the entire text into a vector, where each element in the vector represents a different word.  The vector is the length of the entire vocabulary--usually, every word that occurs in the English language, or at least every word that appears in our corpus.  Any given sentence can then be represented as a vector where all the vector is 1 (or some other value) for each time that word appears in the sentence. \n",
    "\n",
    "Consider the following example: \n",
    "\n",
    "<center>\"I scream, you scream, we all scream for ice cream.\"</center>\n",
    "\n",
    "| 'aardvark' | 'apple' | [...] | 'I' | 'you' | 'scream' | 'we' | 'all' | 'for' | 'ice' | 'cream' | [...] | 'xylophone' | 'zebra' |\n",
    "|:----------:|:-------:|:-----:|:---:|:-----:|:--------:|:----:|:-----:|:-----:|:-----:|:-------:|:-----:|:-----------:|:-------:|\n",
    "|      0     |    0    |   0   |  1  |   1   |     3    |   1  |   1   |   1   |   1   |    1    |   0   |      0      |    0    |\n",
    "\n",
    "This is called a **_Sparse Representation_**, since the strong majority of the columns will have a value of 0.  Note that elements corresponding to words that do not occur in the sentence have a value of 0, while words that do appear in the sentence have a value of 1 (or 1 for each time it appears in the sentence).\n",
    "\n",
    "Alternatively, we can represent this sentence as a plain old python dictionary of word frequency counts:\n",
    "\n",
    "```python\n",
    "BoW = {\n",
    "    'I':1,\n",
    "    'you':1,\n",
    "    'scream':3,\n",
    "    'we':1,\n",
    "    'all':1,\n",
    "    'for':1,\n",
    "    'ice':1,\n",
    "    'cream':1\n",
    "}\n",
    "```\n",
    "\n",
    "Both of these are examples of **_Count Vectorization_**. They allow us to represent a sentence as a vector, with each element in the vector corresponding to how many times that word is used.\n",
    "\n",
    "#### Positional Information and Bag of Words\n",
    "\n",
    "Notice that when we vectorize a sentence this way, we lose the order that the words were in.  This is the **_Bag of Words_** approach mentioned earlier.  Note that sentences that contain the same words will create the same vectors, even if they mean different things--e.g. `'cats are scared of dogs'` and `'dogs are scared of cats'` would both produce the exact same vector, since they contain the same words.  \n",
    "\n",
    "In the cell below, create a function that takes in a tokenized, cleaned song and returns a Count Vectorized representation of it as a python dictionary. Add in an optional parameter called `vocab` that defaults to `None`. This way, if we are using a vocabulary that contains words not seen in the song, we can still use this function by passing it in to the `vocab` parameter. \n",
    "\n",
    "**_Hint:_**  Consider using a `set` object to make this easier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'miss': 3, 'me': 8, 'with': 7, 'that': 10, 'bullshit': 6, 'you': 28, 'ain': 14, 't': 19, 'really': 2, 'wild': 2, 'a': 16, 'tourist': 6, 'i': 66, 'be': 6, 'blackin': 4, 'out': 5, 'the': 22, 'purist': 4, 'made': 6, 'hundred': 3, 'thou': 4, 'then': 10, 'freaked': 14, 'it': 30, '500': 3, 'bought': 3, '87': 2, 'for': 5, 'weekend': 6, 'this': 8, 'what': 13, 'want': 15, 'and': 5, 's': 6, 'like': 9, 'lil': 5, 'bitch': 7, 'mvp': 1, 'get': 9, 'no': 3, 'sleep': 1, 'don': 4, 'bust': 1, 'open': 1, 'ocean': 1, 'yeah': 4, 'bite': 2, 'back': 2, 'do': 2, 'need': 1, 'two': 1, 'life': 3, 'jackets': 1, 'gon': 11, 'hold': 5, 'press': 1, 'never': 5, 'control': 1, 'front': 1, 'keep': 3, '100': 1, 'know': 5, 'boss': 1, 'top': 1, 'dawg': 1, 'bossed': 1, 'my': 19, 'up': 8, 'crossin': 1, 'over': 1, 'stutter': 1, 'steppin': 1, 'got': 12, 'hall': 1, 'of': 1, 'fame': 1, 'in': 6, 'all': 6, 'posters': 1, 've': 1, 'been': 6, 'ready': 9, 'whip': 1, 'clique': 1, 'shit': 1, 'check': 1, 'shot': 1, 'on': 7, 'full': 2, 'armageddon': 1, 'pull': 1, 'hope': 3, 'y': 3, 'tank': 1, 'unleaded': 1, 'ta': 8, 'go': 8, 'name': 2, 'team': 3, 'shots': 1, 'fire': 1, 'roll': 3, 'ménage': 1, 'à': 1, 'trois': 1, 'queen': 1, 'm': 5, 'rain': 1, 'day': 2, 'confetti': 1, 'skrrt': 1, 're': 1, 'not': 18, 'gang': 2, 'member': 1, 'an': 1, '83': 1, 'cutlass': 1, 'thousand': 2, 'put': 2, 'rolls': 1, 'royce': 1, 'wrist': 1, 'oh': 1, 'fuck': 12, 'his': 1, 'baby': 2, 'mama': 1, 'tryna': 1, 'sneak': 1, 'diss': 1, 'took': 1, 'her': 1, 'to': 1, 'penthouse': 1, 'haven': 1, 'mind': 1, 'should': 2, 'big': 1, 'dog': 1, 'status': 1, 'secret': 1, 'la': 2, 'di': 4, 'da': 4, 'slob': 1, 'knob': 1, 'pass': 1, 'some': 1, 'syrup': 1, 'car': 1, 'mothafuck': 1, 'law': 1, 'chitty': 2, 'bang': 1, 'murder': 1, 'everything': 2, 'part': 1, 'ii': 1, 'changes': 1, 'is': 1, 'something': 1, 'red': 6, 'light': 12, 'green': 6, 'they': 3, 'we': 3, 'fast': 5, 'cars': 2, 'money': 1, 'broads': 1, 'egotistic': 1, 'goin': 1, 'ballistic': 1, 'why': 1, 'god': 2, 'born': 1, 'warrior': 1, 'lookin': 1, 'euphoria': 1, 'but': 2, 'see': 1, 'feel': 1, 'paraplegic': 1, 'tapped': 1, 'when': 1, 'maxed': 1, 'compound': 1, 'mac': 1, '10s': 1, 'pumps': 1, 'background': 1, 'was': 4, 'absent': 1, 'og': 1, 'standout': 1, 'lackin': 1, 'else': 1, 'doubt': 1, 'magnum': 2, 'holding': 1, 'magnums': 1, 'nigga': 1, 'ad-lib': 1, 'sing': 1, 'loud': 1, 'had': 3, 'friends': 1, 'ends': 1, '``': 4, 'nope': 1, 'boo': 2, 'yaow': 2, \"''\": 2, 'tee': 1, 'off': 7, 'eat': 1, 'your': 26, 'plate': 1, 'throw': 1, 'ya': 1, 'head': 1, 'well': 1, 'ate': 1, 'c4': 1, 'way': 1, 'edge': 1, 'integrity': 1, 'pedigree': 1, 'feelings': 1, 'culture': 1, 'mom': 1, 'family': 1, 'drive': 1, 'land': 1, 'children': 1, 'wives': 1, 'who': 1, 'am': 1, 'father': 1, 'brother': 1, 'reason': 1, 'future': 1, 'comfort': 1, 'reverence': 1, 'glory': 1, 'heaven': 1, 'angel': 1, 'spirit': 1, 'message': 1, 'freedom': 1, 'people': 1, 'neighbor': 1, 'equal': 1, 'title': 1, 'under': 1, 'hail': 1, 'king': 1, 'killmonger': 1}\n"
     ]
    }
   ],
   "source": [
    "def count_vectorize(song, vocab=None):\n",
    "    \"\"\"\n",
    "    Takes in song lyrics as a list of words\n",
    "    Outputs dictiionary of unique words in list, with \n",
    "        respective counts\n",
    "    \"\"\"    \n",
    "    \n",
    "    #Create a set of the unique words \n",
    "    unique_words = set(song)\n",
    "\n",
    "    # Create a dictionary\n",
    "    word_count = dict.fromkeys(unique_words,0)\n",
    "\n",
    "    #Initialize Dictionary \n",
    "    d={}\n",
    "\n",
    "    # Iterate through the text of Macbeth\n",
    "    for word in song:\n",
    "        d[word] = d.get(word,0)+1\n",
    "    return d  \n",
    "\n",
    "test_vectorized = count_vectorize(tokenized_test_song)\n",
    "print(test_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You've just successfully vectorized your first text document! Now, let's look at a more advanced type of vectorization, TF-IDF!\n",
    "\n",
    "### TF-IDF Vectorization\n",
    "\n",
    "TF-IDF stands for **_Term Frequency, Inverse Document Frequency_**.  This is a more advanced form of vectorization that weights each term in a document by how unique it is to the given document it is contained in, which allows us to summarize the contents of a document using a few key words.  If the word is used often in many other documents, it is not unique, and therefore probably not too useful if we wanted to figure out how this document is unique in relation to other documents.  Conversely, if a word is used many times in a document, but rarely in all the other documents we are considering, then it is likely a good indicator for telling us that this word is important to the document in question.  \n",
    "\n",
    "The formula TF-IDF uses to determine the weights of each term in a document is **_Term Frequency_** multipled by **_Inverse Document Frequency_**, where the formula for Term Frequency is:\n",
    "\n",
    "$$\\large Term\\ Frequency(t) = \\frac{number\\ of\\ times\\ t\\ appears\\ in\\ a\\ document} {total\\ number\\ of\\ terms\\ in\\ the\\ document} $$\n",
    "<br>\n",
    "<br>\n",
    "Complete the following function below to calculate term frequency for every term in a document.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'tourist', 'i', 'be', 'blackin', 'out', 'the', 'purist', 'made', 'hundred']\n"
     ]
    }
   ],
   "source": [
    "def term_frequency(BoW_dict):\n",
    "    \"\"\"\n",
    "    input : a bag of words dictionary, with counts\n",
    "    output : a dictionary with counts replaced with frequency within\n",
    "                document\n",
    "    \"\"\"\n",
    "    tf = {}\n",
    "    \n",
    "    num_terms_doc = sum(BoW_dict.values())\n",
    "    \n",
    "    for word, count in BoW_dict.items():\n",
    "        tf[word]  = count/num_terms_doc\n",
    "    \n",
    "    return tf\n",
    "\n",
    "test = term_frequency(test_vectorized)\n",
    "print(list(test)[10:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for Inverse Document Frequency is:  \n",
    "<br>  \n",
    "<br>\n",
    "$$\\large  IDF(t) =  log_e(\\frac{Total\\ Number\\ of\\ Documents}{Number\\ of\\ Documents\\ with\\ t\\ in\\ it})$$\n",
    "\n",
    "Now that we have this, we can easily calculate _Inverse Document Frequency_.  In the cell below, complete the following function.  this function should take in the list of dictionaries, with each item in the list being a Bag of Words representing the words in a different song. The function should return a dictionary containing the inverse document frequency values for each word.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_document_frequency(list_of_dicts):\n",
    "    # Instantiate set\n",
    "    all_words = set()\n",
    "   \n",
    "    # Fill set with unique words\n",
    "    for song in list_of_dicts:\n",
    "        for word in song.keys():\n",
    "            all_words.add(word)\n",
    "    \n",
    "    # Based on set, create base dictionary with 0 counts\n",
    "    # Let's use dictionary comprehension\n",
    "    all_words_dict = {word:0 for word in all_words}\n",
    "    \n",
    "    # Loop through all_words\n",
    "    for word, count in all_words_dict.items():\n",
    "        \n",
    "        curr_word_ct = 0\n",
    "        \n",
    "        # Loop through each song, get words:\n",
    "        for song in list_of_dicts:\n",
    "            if word in song:\n",
    "                curr_word_ct += 1\n",
    "\n",
    "        all_words_dict[word] = np.log(len(list_of_dicts)/curr_word_ct)\n",
    "        \n",
    "    return all_words_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'second': 2.302585092994046,\n",
       " 'afraid': 2.995732273553991,\n",
       " 'man': 1.2039728043259361,\n",
       " 'mando': 2.995732273553991,\n",
       " 'freedom': 2.302585092994046,\n",
       " 'whoever': 2.995732273553991,\n",
       " 'dirt': 2.995732273553991,\n",
       " 'head': 1.6094379124341003,\n",
       " 'people': 2.302585092994046,\n",
       " 'hey': 2.302585092994046,\n",
       " 'team': 2.302585092994046,\n",
       " 'wrong': 2.302585092994046,\n",
       " 'brother': 2.302585092994046,\n",
       " 'herb': 2.995732273553991,\n",
       " 'realest': 2.995732273553991,\n",
       " 'friend': 2.995732273553991,\n",
       " 'matches': 2.995732273553991,\n",
       " 'hope': 1.8971199848858813,\n",
       " 'lose': 2.995732273553991,\n",
       " 'kill': 2.302585092994046,\n",
       " 'cleaver': 2.995732273553991,\n",
       " 'heading': 2.995732273553991,\n",
       " 'doors': 2.302585092994046,\n",
       " 'teach': 2.995732273553991,\n",
       " 'serve': 2.995732273553991,\n",
       " 'drummer': 2.995732273553991,\n",
       " 'been': 1.0498221244986776,\n",
       " 'sound': 2.995732273553991,\n",
       " 'fingerprints': 2.995732273553991,\n",
       " 'yeah': 1.0498221244986776,\n",
       " 'trying': 1.8971199848858813,\n",
       " 'sing': 2.995732273553991,\n",
       " 'neighbor': 2.995732273553991,\n",
       " 'surface': 2.995732273553991,\n",
       " 'warfare': 2.995732273553991,\n",
       " 'own': 1.8971199848858813,\n",
       " 'thumpin': 2.995732273553991,\n",
       " 'full': 2.995732273553991,\n",
       " 'fate': 2.995732273553991,\n",
       " 'world': 1.3862943611198906,\n",
       " 'to—': 2.995732273553991,\n",
       " 'behind': 2.995732273553991,\n",
       " 'road': 1.8971199848858813,\n",
       " 'pick-up': 2.995732273553991,\n",
       " 'high': 2.302585092994046,\n",
       " 'night': 1.3862943611198906,\n",
       " 'who': 0.7985076962177716,\n",
       " 'ever': 2.302585092994046,\n",
       " 'hard': 1.8971199848858813,\n",
       " 'angel': 2.302585092994046,\n",
       " 'rubber': 2.995732273553991,\n",
       " 'stealing': 2.995732273553991,\n",
       " 'woah': 2.995732273553991,\n",
       " 'fly': 2.995732273553991,\n",
       " 'ye': 2.995732273553991,\n",
       " 'psych': 2.995732273553991,\n",
       " 'bangs': 2.995732273553991,\n",
       " 'words': 2.995732273553991,\n",
       " 'heart': 1.6094379124341003,\n",
       " 'ego': 2.995732273553991,\n",
       " 'phillipe': 2.995732273553991,\n",
       " 'hail': 2.302585092994046,\n",
       " 'welcome': 2.995732273553991,\n",
       " 'simply': 2.995732273553991,\n",
       " 'forget': 2.302585092994046,\n",
       " 'anything': 1.8971199848858813,\n",
       " 'waited': 2.995732273553991,\n",
       " 'wouldn': 2.995732273553991,\n",
       " 'pretty': 2.995732273553991,\n",
       " 'stars': 2.995732273553991,\n",
       " 'yourself': 2.995732273553991,\n",
       " 'is': 0.22314355131420976,\n",
       " 'brain': 2.995732273553991,\n",
       " 'can': 1.0498221244986776,\n",
       " 'figures': 2.995732273553991,\n",
       " 'knees': 2.995732273553991,\n",
       " 'flowers': 2.995732273553991,\n",
       " 'crime': 2.995732273553991,\n",
       " 'explain': 2.995732273553991,\n",
       " 'about': 1.2039728043259361,\n",
       " 'rebound': 2.995732273553991,\n",
       " 'lawless': 2.995732273553991,\n",
       " 'hopin': 2.995732273553991,\n",
       " 'found': 2.995732273553991,\n",
       " 'scenes': 2.995732273553991,\n",
       " 'lent': 2.995732273553991,\n",
       " 'find': 1.6094379124341003,\n",
       " 'cover': 2.995732273553991,\n",
       " '8teen': 2.995732273553991,\n",
       " 'ain': 0.9162907318741551,\n",
       " 'drinks': 2.995732273553991,\n",
       " 'kings': 2.995732273553991,\n",
       " 'plenty': 2.995732273553991,\n",
       " 'bout': 2.302585092994046,\n",
       " 'figure': 2.995732273553991,\n",
       " 'building': 2.302585092994046,\n",
       " 'left': 2.995732273553991,\n",
       " 'makin': 2.302585092994046,\n",
       " 'spit': 2.995732273553991,\n",
       " 'punch': 2.995732273553991,\n",
       " 'equal': 2.995732273553991,\n",
       " 'leave': 2.302585092994046,\n",
       " 'were': 2.995732273553991,\n",
       " 'd': 1.6094379124341003,\n",
       " 'shotgun': 2.995732273553991,\n",
       " 'romantic': 2.995732273553991,\n",
       " 'ayy': 2.995732273553991,\n",
       " 'space': 2.302585092994046,\n",
       " 'family': 2.995732273553991,\n",
       " 'surprised': 2.995732273553991,\n",
       " 'please': 2.995732273553991,\n",
       " 'ventura': 2.995732273553991,\n",
       " 'gon': 0.7985076962177716,\n",
       " 'rhyme': 2.995732273553991,\n",
       " 'sometimes': 2.995732273553991,\n",
       " 'tryin': 2.995732273553991,\n",
       " 'à': 2.995732273553991,\n",
       " 'metaphor': 2.995732273553991,\n",
       " 'again': 1.6094379124341003,\n",
       " 'system': 2.995732273553991,\n",
       " 'benz': 2.995732273553991,\n",
       " 'emotional': 2.995732273553991,\n",
       " 'mirror': 2.995732273553991,\n",
       " 'damn': 2.302585092994046,\n",
       " 'ohh': 2.995732273553991,\n",
       " 'ward': 2.995732273553991,\n",
       " 'than': 2.302585092994046,\n",
       " 'credit': 2.995732273553991,\n",
       " 'style': 2.302585092994046,\n",
       " 'hours': 2.995732273553991,\n",
       " 'role': 2.995732273553991,\n",
       " 'missing': 2.995732273553991,\n",
       " 'body': 1.6094379124341003,\n",
       " '87': 2.995732273553991,\n",
       " 'no': 0.7985076962177716,\n",
       " 'well': 2.302585092994046,\n",
       " 'lil': 2.995732273553991,\n",
       " 'hoe': 2.995732273553991,\n",
       " 'compound': 2.995732273553991,\n",
       " 'white': 2.995732273553991,\n",
       " 'ceiling': 2.995732273553991,\n",
       " 'vincent': 2.995732273553991,\n",
       " 'unleash': 2.995732273553991,\n",
       " 'color': 2.995732273553991,\n",
       " 'drive': 2.302585092994046,\n",
       " 'six': 2.995732273553991,\n",
       " 'will': 1.8971199848858813,\n",
       " 'wrist': 2.995732273553991,\n",
       " 'that': 0.16251892949777494,\n",
       " 'don': 0.3566749439387324,\n",
       " 'stand': 2.995732273553991,\n",
       " 'state': 2.995732273553991,\n",
       " 'lets': 2.995732273553991,\n",
       " 'brand': 2.302585092994046,\n",
       " 'strike': 2.995732273553991,\n",
       " 'bodies': 2.995732273553991,\n",
       " 'everybody': 2.302585092994046,\n",
       " 'target': 2.995732273553991,\n",
       " 'mom': 2.995732273553991,\n",
       " 'falls': 2.995732273553991,\n",
       " 'stopping': 2.995732273553991,\n",
       " 'listens': 2.995732273553991,\n",
       " 'peanut': 2.995732273553991,\n",
       " 'nerve': 2.995732273553991,\n",
       " 'ralph': 2.995732273553991,\n",
       " 'western': 2.995732273553991,\n",
       " 'petty': 2.995732273553991,\n",
       " 'baby': 1.2039728043259361,\n",
       " 'sad': 2.995732273553991,\n",
       " 'nowhere': 1.8971199848858813,\n",
       " 'only': 2.302585092994046,\n",
       " 'watch': 2.995732273553991,\n",
       " 'change': 2.302585092994046,\n",
       " 'today': 2.302585092994046,\n",
       " 'lift': 2.995732273553991,\n",
       " 'green': 2.302585092994046,\n",
       " 'pow': 2.995732273553991,\n",
       " 'moat': 2.995732273553991,\n",
       " 'channel': 2.995732273553991,\n",
       " 'lights': 2.995732273553991,\n",
       " 'racks': 2.995732273553991,\n",
       " 'day': 1.0498221244986776,\n",
       " 'balance': 2.995732273553991,\n",
       " 'skip': 2.995732273553991,\n",
       " 'tendons': 2.995732273553991,\n",
       " 'party': 1.8971199848858813,\n",
       " 'tend': 2.995732273553991,\n",
       " 'gates': 2.995732273553991,\n",
       " 'general': 2.995732273553991,\n",
       " 'defeat': 2.995732273553991,\n",
       " 'isn': 2.995732273553991,\n",
       " 'shot': 2.302585092994046,\n",
       " 'nice': 2.995732273553991,\n",
       " 'boat': 2.995732273553991,\n",
       " 'all': 0.22314355131420976,\n",
       " 'make': 1.3862943611198906,\n",
       " 'prowl': 2.995732273553991,\n",
       " 'hummed': 2.995732273553991,\n",
       " 'such': 2.995732273553991,\n",
       " 'sin': 2.995732273553991,\n",
       " 'vibrant': 2.995732273553991,\n",
       " 'yugen': 2.995732273553991,\n",
       " 'problem': 2.302585092994046,\n",
       " 'nothing': 2.302585092994046,\n",
       " 'louisiana': 2.995732273553991,\n",
       " 'strength': 2.995732273553991,\n",
       " 'squad': 2.995732273553991,\n",
       " 'tee': 2.995732273553991,\n",
       " 'jack': 2.302585092994046,\n",
       " 'under': 2.302585092994046,\n",
       " 'gosh': 2.995732273553991,\n",
       " 'warrior': 2.995732273553991,\n",
       " 'everywhere': 2.995732273553991,\n",
       " 'water': 2.302585092994046,\n",
       " 'jump': 2.302585092994046,\n",
       " 'livin': 2.302585092994046,\n",
       " 'stick': 2.302585092994046,\n",
       " 'polite': 2.995732273553991,\n",
       " 'come': 2.302585092994046,\n",
       " 'smile': 2.995732273553991,\n",
       " 'congratulations': 2.995732273553991,\n",
       " 'freaked': 2.995732273553991,\n",
       " 'insides': 2.995732273553991,\n",
       " 'zero': 2.995732273553991,\n",
       " 'bag': 2.995732273553991,\n",
       " 'jury': 2.995732273553991,\n",
       " 'shots': 2.302585092994046,\n",
       " 'case': 2.995732273553991,\n",
       " 'wait': 1.8971199848858813,\n",
       " 'kinda': 2.995732273553991,\n",
       " 'nigga': 1.3862943611198906,\n",
       " 'claimed': 2.995732273553991,\n",
       " 'butter': 2.995732273553991,\n",
       " 'power': 2.995732273553991,\n",
       " 'rover': 2.995732273553991,\n",
       " 'pity': 2.995732273553991,\n",
       " 'planes': 2.995732273553991,\n",
       " 'blessing': 2.995732273553991,\n",
       " 'storm': 2.995732273553991,\n",
       " 'juggin': 2.995732273553991,\n",
       " 'had': 1.6094379124341003,\n",
       " 'agua': 2.995732273553991,\n",
       " 'kalashnikov': 2.995732273553991,\n",
       " 'involved': 2.995732273553991,\n",
       " 'begins': 2.302585092994046,\n",
       " 'blackin': 2.995732273553991,\n",
       " 'gift': 2.302585092994046,\n",
       " 'machine': 2.995732273553991,\n",
       " 'probably': 2.302585092994046,\n",
       " 'depart': 2.995732273553991,\n",
       " 'jc': 2.995732273553991,\n",
       " 'top': 1.6094379124341003,\n",
       " 'sorry': 2.995732273553991,\n",
       " 'sho': 2.995732273553991,\n",
       " 'tank': 2.995732273553991,\n",
       " 'ride': 1.8971199848858813,\n",
       " 'oh': 1.3862943611198906,\n",
       " 'talk': 1.8971199848858813,\n",
       " 'one': 1.2039728043259361,\n",
       " 'places': 2.995732273553991,\n",
       " 'something': 0.7985076962177716,\n",
       " 'game': 2.302585092994046,\n",
       " 'vince': 2.995732273553991,\n",
       " 'drown': 2.302585092994046,\n",
       " 'hall': 2.995732273553991,\n",
       " 'ropes': 2.995732273553991,\n",
       " 'wreck': 2.995732273553991,\n",
       " 'lay': 2.995732273553991,\n",
       " 'hurts': 2.995732273553991,\n",
       " 'wherever': 2.995732273553991,\n",
       " 'thirty': 2.995732273553991,\n",
       " 'happy': 2.995732273553991,\n",
       " '83': 2.995732273553991,\n",
       " 'way': 1.2039728043259361,\n",
       " 'taxi': 2.995732273553991,\n",
       " 'small': 2.302585092994046,\n",
       " 'ear': 2.995732273553991,\n",
       " 'standing': 2.302585092994046,\n",
       " 'making': 2.995732273553991,\n",
       " 'title': 2.995732273553991,\n",
       " 'topped-off': 2.995732273553991,\n",
       " 'euphoria': 2.995732273553991,\n",
       " 'picture': 2.995732273553991,\n",
       " 'straight': 2.995732273553991,\n",
       " 'really': 1.8971199848858813,\n",
       " 'momma': 2.995732273553991,\n",
       " 'hero': 2.995732273553991,\n",
       " 'at': 1.0498221244986776,\n",
       " 'hell': 2.302585092994046,\n",
       " 'starts': 2.302585092994046,\n",
       " 'check': 2.302585092994046,\n",
       " 'pristine': 2.995732273553991,\n",
       " 's': 0.0,\n",
       " 'hiding': 2.995732273553991,\n",
       " 'lanes': 2.995732273553991,\n",
       " 'big': 1.3862943611198906,\n",
       " 'armory': 2.995732273553991,\n",
       " 'do': 0.7985076962177716,\n",
       " 'hand': 2.302585092994046,\n",
       " '&': 2.995732273553991,\n",
       " 'lady': 2.302585092994046,\n",
       " 'him': 2.302585092994046,\n",
       " 'may': 2.302585092994046,\n",
       " 'ate': 2.995732273553991,\n",
       " 'stays': 2.995732273553991,\n",
       " 'west': 2.995732273553991,\n",
       " 'government': 2.995732273553991,\n",
       " 'swimming': 2.995732273553991,\n",
       " 'shooters': 2.995732273553991,\n",
       " 'views': 2.995732273553991,\n",
       " 'subject': 2.995732273553991,\n",
       " 'sweet': 2.302585092994046,\n",
       " 'girl': 1.2039728043259361,\n",
       " 'knew': 2.995732273553991,\n",
       " 'south': 2.995732273553991,\n",
       " 'father': 2.995732273553991,\n",
       " 'dessert': 2.995732273553991,\n",
       " 'raw': 2.995732273553991,\n",
       " 'spending': 2.995732273553991,\n",
       " 'would': 1.8971199848858813,\n",
       " 'blakrok': 2.995732273553991,\n",
       " 'needle': 2.995732273553991,\n",
       " 'trapped': 2.995732273553991,\n",
       " 'guys': 2.995732273553991,\n",
       " 'crash': 2.995732273553991,\n",
       " 'dropped': 2.995732273553991,\n",
       " 'hurricanes': 2.995732273553991,\n",
       " 'mind': 1.6094379124341003,\n",
       " 're': 0.6931471805599453,\n",
       " 'broken': 2.995732273553991,\n",
       " 'glowin': 2.995732273553991,\n",
       " 'cryin': 2.995732273553991,\n",
       " 'ah': 2.995732273553991,\n",
       " 'bitch': 2.302585092994046,\n",
       " 'long': 2.302585092994046,\n",
       " 'piece': 1.8971199848858813,\n",
       " 'juice': 2.995732273553991,\n",
       " 'better': 1.3862943611198906,\n",
       " 'winnin': 2.995732273553991,\n",
       " 'we': 0.5108256237659907,\n",
       " 'write': 2.995732273553991,\n",
       " 'stop': 2.995732273553991,\n",
       " 'pearly': 2.995732273553991,\n",
       " 'posters': 2.995732273553991,\n",
       " 'from': 0.6931471805599453,\n",
       " 'yah': 2.995732273553991,\n",
       " 'cabanas': 2.995732273553991,\n",
       " 'lamar': 2.995732273553991,\n",
       " 'diss': 2.995732273553991,\n",
       " 'mass': 2.995732273553991,\n",
       " 'kickin': 2.995732273553991,\n",
       " 'dancing': 2.995732273553991,\n",
       " 'pumps': 2.995732273553991,\n",
       " 'ready': 1.6094379124341003,\n",
       " 'cross': 2.995732273553991,\n",
       " 'secret': 2.995732273553991,\n",
       " 'act': 2.995732273553991,\n",
       " 'gone': 2.995732273553991,\n",
       " 'debt': 2.995732273553991,\n",
       " 'bambe': 2.995732273553991,\n",
       " 'truly': 2.995732273553991,\n",
       " 'for': 0.43078291609245434,\n",
       " 'sun': 2.995732273553991,\n",
       " 'blonde': 2.995732273553991,\n",
       " 'shoot': 2.302585092994046,\n",
       " 'whippin': 2.995732273553991,\n",
       " 'rolls': 2.995732273553991,\n",
       " 'og': 2.995732273553991,\n",
       " 'rejoin': 2.995732273553991,\n",
       " 'uncle': 2.995732273553991,\n",
       " 'lines': 2.995732273553991,\n",
       " 'ways': 1.6094379124341003,\n",
       " 'coming': 2.995732273553991,\n",
       " 'worth': 2.302585092994046,\n",
       " 'slob': 2.995732273553991,\n",
       " '20': 2.995732273553991,\n",
       " 'fiddy': 2.995732273553991,\n",
       " 'plus': 2.995732273553991,\n",
       " 'hurtin': 2.995732273553991,\n",
       " 'rodeo': 2.995732273553991,\n",
       " 'home': 1.8971199848858813,\n",
       " 'ends': 2.302585092994046,\n",
       " 'loves': 2.995732273553991,\n",
       " 'standin': 2.995732273553991,\n",
       " 'someday': 2.995732273553991,\n",
       " '*censored*': 2.995732273553991,\n",
       " 'magnum': 2.995732273553991,\n",
       " 'in': 0.10536051565782635,\n",
       " 'gentiles': 2.995732273553991,\n",
       " 'desperate': 2.995732273553991,\n",
       " 'rollin': 2.302585092994046,\n",
       " 'blind': 2.302585092994046,\n",
       " 'thirstin': 2.995732273553991,\n",
       " 'sweating': 2.995732273553991,\n",
       " 'figurines': 2.995732273553991,\n",
       " 'gets': 2.302585092994046,\n",
       " 'stretch': 2.995732273553991,\n",
       " 'look': 1.3862943611198906,\n",
       " 'swear': 1.8971199848858813,\n",
       " 'born': 2.302585092994046,\n",
       " 'with': 0.6931471805599453,\n",
       " 'unleaded': 2.995732273553991,\n",
       " 'must': 2.995732273553991,\n",
       " 'magnums': 2.995732273553991,\n",
       " 'king': 2.995732273553991,\n",
       " 'grabs': 2.995732273553991,\n",
       " 'member': 2.995732273553991,\n",
       " 'o': 2.995732273553991,\n",
       " 'looks': 2.995732273553991,\n",
       " 'trinity': 2.995732273553991,\n",
       " 'more': 1.6094379124341003,\n",
       " 'stutter': 2.995732273553991,\n",
       " 'dedele': 2.995732273553991,\n",
       " 'know': 0.3566749439387324,\n",
       " 'cup': 2.995732273553991,\n",
       " 'lasers': 2.995732273553991,\n",
       " 'true': 2.995732273553991,\n",
       " 'slow': 2.995732273553991,\n",
       " 'getting': 2.995732273553991,\n",
       " 'remote': 2.995732273553991,\n",
       " 'japan': 2.995732273553991,\n",
       " 'armored': 2.995732273553991,\n",
       " 'car': 1.6094379124341003,\n",
       " 'try': 1.6094379124341003,\n",
       " 'two': 1.3862943611198906,\n",
       " 'runneth': 2.995732273553991,\n",
       " 'ask': 2.995732273553991,\n",
       " 'aching': 2.995732273553991,\n",
       " 'swore': 2.302585092994046,\n",
       " 'beams': 2.995732273553991,\n",
       " '4': 2.995732273553991,\n",
       " 'lane': 2.995732273553991,\n",
       " 'ice': 2.995732273553991,\n",
       " 'antidote': 2.995732273553991,\n",
       " 'haunt': 2.995732273553991,\n",
       " 'thieving': 2.995732273553991,\n",
       " 'kept': 2.302585092994046,\n",
       " 'confrontation': 2.995732273553991,\n",
       " 'stubborn': 2.995732273553991,\n",
       " 'underlined': 2.995732273553991,\n",
       " 'verbal': 2.995732273553991,\n",
       " 'obscene': 2.995732273553991,\n",
       " 'tonight': 2.995732273553991,\n",
       " 'law': 2.302585092994046,\n",
       " 'purist': 2.995732273553991,\n",
       " 'niggas': 2.302585092994046,\n",
       " 'lives': 2.995732273553991,\n",
       " 'real': 2.302585092994046,\n",
       " 'but': 0.3566749439387324,\n",
       " 'cash': 2.995732273553991,\n",
       " 'aimin': 2.995732273553991,\n",
       " 'bouta': 2.995732273553991,\n",
       " 'lord': 2.302585092994046,\n",
       " 'benji': 2.995732273553991,\n",
       " 'criminal': 2.302585092994046,\n",
       " 'letters': 2.995732273553991,\n",
       " 'suit': 2.995732273553991,\n",
       " 'flash': 2.995732273553991,\n",
       " 'uh': 2.995732273553991,\n",
       " 'nursin': 2.995732273553991,\n",
       " 'swingin': 2.995732273553991,\n",
       " 'satisfied': 2.995732273553991,\n",
       " 'trippin': 2.995732273553991,\n",
       " 'spilled': 2.995732273553991,\n",
       " 'da': 2.995732273553991,\n",
       " 'grindin': 2.995732273553991,\n",
       " 'fiery': 2.995732273553991,\n",
       " 'goes': 2.995732273553991,\n",
       " 'mothafuck': 2.995732273553991,\n",
       " 'deaf': 2.995732273553991,\n",
       " 'purped': 2.995732273553991,\n",
       " 'an': 1.6094379124341003,\n",
       " 'sacrificed': 2.995732273553991,\n",
       " 'rules': 2.995732273553991,\n",
       " 'walls': 2.995732273553991,\n",
       " 'blood': 2.302585092994046,\n",
       " 'handsome': 2.995732273553991,\n",
       " 'else': 2.302585092994046,\n",
       " 'guess': 2.995732273553991,\n",
       " 'off': 1.2039728043259361,\n",
       " 'percentage': 2.995732273553991,\n",
       " 'finesse': 2.995732273553991,\n",
       " 'let': 1.0498221244986776,\n",
       " 'brothers': 2.995732273553991,\n",
       " 'aura': 2.995732273553991,\n",
       " 'common': 2.995732273553991,\n",
       " 'valentine': 2.995732273553991,\n",
       " 'red': 1.8971199848858813,\n",
       " 'fucked': 2.995732273553991,\n",
       " 'burning': 2.302585092994046,\n",
       " 'these': 2.995732273553991,\n",
       " 'em': 1.6094379124341003,\n",
       " 'skin': 2.995732273553991,\n",
       " 'jo-ann': 2.995732273553991,\n",
       " 'steppin': 2.995732273553991,\n",
       " 'queens': 2.995732273553991,\n",
       " 'model': 2.995732273553991,\n",
       " 'cyborg': 2.995732273553991,\n",
       " 'me': 0.22314355131420976,\n",
       " 'seen': 2.302585092994046,\n",
       " 'hittas': 2.995732273553991,\n",
       " 'lows': 2.995732273553991,\n",
       " 'hundred': 2.995732273553991,\n",
       " 'feeling': 1.6094379124341003,\n",
       " 'feel': 1.8971199848858813,\n",
       " 'stack': 2.995732273553991,\n",
       " 'shit': 1.8971199848858813,\n",
       " 'worse': 2.995732273553991,\n",
       " 'invite': 2.995732273553991,\n",
       " 'ung': 2.995732273553991,\n",
       " 'hoped': 2.995732273553991,\n",
       " 'swing': 2.995732273553991,\n",
       " 'sittin': 2.995732273553991,\n",
       " 'letter': 2.995732273553991,\n",
       " 'sailing': 2.995732273553991,\n",
       " 'diddy': 2.995732273553991,\n",
       " 'many': 2.302585092994046,\n",
       " 'talkin': 2.995732273553991,\n",
       " 'electorial': 2.995732273553991,\n",
       " 'bypass': 2.995732273553991,\n",
       " 'slide': 2.995732273553991,\n",
       " 'montana': 2.995732273553991,\n",
       " 'new': 1.8971199848858813,\n",
       " 'hoping': 2.995732273553991,\n",
       " 'nobody': 2.995732273553991,\n",
       " 'extensions': 2.995732273553991,\n",
       " 'false': 2.995732273553991,\n",
       " 'castin': 2.995732273553991,\n",
       " 'keep': 1.3862943611198906,\n",
       " 'busting': 2.995732273553991,\n",
       " 'weekend': 2.302585092994046,\n",
       " 'still': 1.3862943611198906,\n",
       " 'spent': 2.995732273553991,\n",
       " 'losin': 2.995732273553991,\n",
       " 'money': 1.8971199848858813,\n",
       " 'half': 2.995732273553991,\n",
       " 'beer': 2.995732273553991,\n",
       " 'sees': 2.302585092994046,\n",
       " 'fourteen': 2.995732273553991,\n",
       " 'morning': 2.995732273553991,\n",
       " 'pure': 2.995732273553991,\n",
       " 'crushing': 2.995732273553991,\n",
       " '10s': 2.995732273553991,\n",
       " 'righteous': 2.995732273553991,\n",
       " 'ooh': 2.995732273553991,\n",
       " 'cowboys': 2.995732273553991,\n",
       " 'jukejoint': 2.995732273553991,\n",
       " 'when': 0.3566749439387324,\n",
       " 'quite': 2.995732273553991,\n",
       " 'streets': 2.302585092994046,\n",
       " 'absent': 2.995732273553991,\n",
       " 'highway': 2.302585092994046,\n",
       " 'shows': 2.995732273553991,\n",
       " 'swaps': 2.995732273553991,\n",
       " 'killmonger': 2.995732273553991,\n",
       " 'wild': 1.8971199848858813,\n",
       " 'some': 1.6094379124341003,\n",
       " 'amen': 1.8971199848858813,\n",
       " 'armageddon': 2.995732273553991,\n",
       " 'syrup': 2.302585092994046,\n",
       " 'a': 0.16251892949777494,\n",
       " 'like': 0.43078291609245434,\n",
       " 'taping': 2.995732273553991,\n",
       " 'haven': 2.995732273553991,\n",
       " 'lioness': 2.995732273553991,\n",
       " 'good': 1.3862943611198906,\n",
       " 'girls': 2.995732273553991,\n",
       " 'keys': 2.995732273553991,\n",
       " 'center-attention': 2.995732273553991,\n",
       " 'over': 1.3862943611198906,\n",
       " 'standby': 2.995732273553991,\n",
       " 'confidence': 2.995732273553991,\n",
       " 'kikiritikiki': 2.995732273553991,\n",
       " 'tips': 2.995732273553991,\n",
       " 'la': 2.995732273553991,\n",
       " 'lackin': 2.995732273553991,\n",
       " 'then': 1.6094379124341003,\n",
       " 'coast': 2.995732273553991,\n",
       " 'slick': 2.995732273553991,\n",
       " 'any': 2.995732273553991,\n",
       " 'changed': 2.995732273553991,\n",
       " 'wives': 2.995732273553991,\n",
       " 'settle': 2.995732273553991,\n",
       " 'together': 2.302585092994046,\n",
       " 'rockin': 2.995732273553991,\n",
       " 'pedigree': 2.995732273553991,\n",
       " 'jumpin': 2.995732273553991,\n",
       " 'zone': 2.995732273553991,\n",
       " 'whoa': 2.995732273553991,\n",
       " 'street': 2.995732273553991,\n",
       " 'another': 1.8971199848858813,\n",
       " 'passed': 2.995732273553991,\n",
       " 'mama': 2.995732273553991,\n",
       " 'whiskey': 2.995732273553991,\n",
       " 'ya': 1.6094379124341003,\n",
       " 'runnin': 1.3862943611198906,\n",
       " 'chance': 2.302585092994046,\n",
       " 'chords': 2.995732273553991,\n",
       " 'was': 1.3862943611198906,\n",
       " 'throne': 2.995732273553991,\n",
       " 'first': 2.995732273553991,\n",
       " 've': 1.2039728043259361,\n",
       " 'standout': 2.995732273553991,\n",
       " 'pick': 2.995732273553991,\n",
       " 'turned': 2.995732273553991,\n",
       " 'run': 1.8971199848858813,\n",
       " 'cutlass': 2.995732273553991,\n",
       " 'belittles': 2.995732273553991,\n",
       " 'y': 2.302585092994046,\n",
       " 'around': 2.302585092994046,\n",
       " 'del': 2.995732273553991,\n",
       " 'raging': 2.995732273553991,\n",
       " 'fuck': 1.6094379124341003,\n",
       " 'dawg': 2.302585092994046,\n",
       " 'ship': 2.995732273553991,\n",
       " 'hop': 2.995732273553991,\n",
       " 'bullet': 2.302585092994046,\n",
       " 'desire': 2.995732273553991,\n",
       " 'what': 0.5108256237659907,\n",
       " 'depending': 2.995732273553991,\n",
       " 'bought': 2.302585092994046,\n",
       " 'dreams': 1.8971199848858813,\n",
       " 'spitting': 2.995732273553991,\n",
       " 'earthquake': 2.995732273553991,\n",
       " 'say': 1.0498221244986776,\n",
       " 'put': 1.6094379124341003,\n",
       " 'everyone': 2.302585092994046,\n",
       " 'conversation': 2.995732273553991,\n",
       " 'coons': 2.995732273553991,\n",
       " 'knocks': 2.995732273553991,\n",
       " 'hold': 1.6094379124341003,\n",
       " 'night-call': 2.995732273553991,\n",
       " 'iggy': 2.995732273553991,\n",
       " 'call': 1.6094379124341003,\n",
       " 'doesn': 2.302585092994046,\n",
       " 'bend': 2.995732273553991,\n",
       " 'cab': 2.995732273553991,\n",
       " 'gotham': 2.995732273553991,\n",
       " 'help': 2.995732273553991,\n",
       " 'answer': 2.995732273553991,\n",
       " 'has': 2.995732273553991,\n",
       " 'blades': 2.995732273553991,\n",
       " 'feels': 2.995732273553991,\n",
       " 'time': 0.5978370007556204,\n",
       " 'prophecy': 2.995732273553991,\n",
       " 'refined': 2.995732273553991,\n",
       " 'war': 2.302585092994046,\n",
       " 'buildin': 2.995732273553991,\n",
       " 'clock': 2.995732273553991,\n",
       " 'trance': 2.995732273553991,\n",
       " 'carryin': 2.995732273553991,\n",
       " 'lookin': 2.995732273553991,\n",
       " 'recognize': 2.995732273553991,\n",
       " 'knowing': 2.302585092994046,\n",
       " 'bring': 2.302585092994046,\n",
       " 't': 0.10536051565782635,\n",
       " 'those': 2.995732273553991,\n",
       " 'dirty': 2.995732273553991,\n",
       " 'sign': 2.995732273553991,\n",
       " 'future': 2.995732273553991,\n",
       " 'steve': 2.995732273553991,\n",
       " 'guard': 2.995732273553991,\n",
       " 'said': 2.302585092994046,\n",
       " 'burdens': 2.995732273553991,\n",
       " 'open': 1.3862943611198906,\n",
       " 'wins': 2.995732273553991,\n",
       " 'later': 2.995732273553991,\n",
       " '100': 2.995732273553991,\n",
       " 'name': 1.8971199848858813,\n",
       " 'wave': 2.995732273553991,\n",
       " 'whip': 2.995732273553991,\n",
       " 'and': 0.05129329438755048,\n",
       " 'crazy': 1.3862943611198906,\n",
       " 'weeknd': 2.995732273553991,\n",
       " 'break': 2.995732273553991,\n",
       " 'breathin': 2.995732273553991,\n",
       " 'siri': 2.995732273553991,\n",
       " 'mouth': 2.995732273553991,\n",
       " 'anywhere': 2.302585092994046,\n",
       " 'somebody': 2.995732273553991,\n",
       " 'borderline': 2.995732273553991,\n",
       " 'forever': 2.302585092994046,\n",
       " 'kitchen': 2.995732273553991,\n",
       " 'knows': 1.8971199848858813,\n",
       " 'them': 1.6094379124341003,\n",
       " 'soul': 2.995732273553991,\n",
       " 'plate': 2.302585092994046,\n",
       " 'loving': 2.995732273553991,\n",
       " 'children': 2.995732273553991,\n",
       " 'last': 2.302585092994046,\n",
       " 'corruption': 2.995732273553991,\n",
       " 'committing': 2.995732273553991,\n",
       " 'clutchin': 2.995732273553991,\n",
       " 'face': 1.6094379124341003,\n",
       " 'mayhem': 2.995732273553991,\n",
       " 'sufferin': 2.995732273553991,\n",
       " \"''\": 1.6094379124341003,\n",
       " 'chess': 2.995732273553991,\n",
       " 'pray': 2.302585092994046,\n",
       " 'know-ow': 2.995732273553991,\n",
       " 'reason': 2.302585092994046,\n",
       " 'aftermath': 2.995732273553991,\n",
       " 'throw': 2.995732273553991,\n",
       " 'doorsteps': 2.995732273553991,\n",
       " 'want': 1.2039728043259361,\n",
       " 'dying': 2.302585092994046,\n",
       " 'move': 1.3862943611198906,\n",
       " 'closed': 2.995732273553991,\n",
       " 'hazy': 2.995732273553991,\n",
       " 'roll': 1.8971199848858813,\n",
       " 'wrongs': 2.995732273553991,\n",
       " 'hers': 2.995732273553991,\n",
       " 'patient': 2.995732273553991,\n",
       " 'important': 2.995732273553991,\n",
       " 'opinion': 2.995732273553991,\n",
       " 'pop': 2.995732273553991,\n",
       " 'scott': 2.995732273553991,\n",
       " 'n': 2.995732273553991,\n",
       " 'feelings': 2.995732273553991,\n",
       " 'right': 1.2039728043259361,\n",
       " 'turn': 2.302585092994046,\n",
       " 'track': 2.995732273553991,\n",
       " 'am': 1.8971199848858813,\n",
       " 'sundown': 2.995732273553991,\n",
       " 'rode': 2.995732273553991,\n",
       " 'ground': 2.302585092994046,\n",
       " 'college': 2.995732273553991,\n",
       " 'machinery': 2.995732273553991,\n",
       " 'knob': 2.995732273553991,\n",
       " 'perimeter': 2.995732273553991,\n",
       " 'amo': 2.995732273553991,\n",
       " 'dial': 2.995732273553991,\n",
       " 'crawl': 2.995732273553991,\n",
       " 'here': 1.2039728043259361,\n",
       " 'russians': 2.995732273553991,\n",
       " 'buggin': 2.995732273553991,\n",
       " 'blame': 2.995732273553991,\n",
       " 'working': 2.995732273553991,\n",
       " 'front': 2.302585092994046,\n",
       " 'mines': 2.995732273553991,\n",
       " 'lemon': 2.995732273553991,\n",
       " 'bang': 2.302585092994046,\n",
       " 'are': 1.0498221244986776,\n",
       " 'hi-rez': 2.995732273553991,\n",
       " 'promises': 2.995732273553991,\n",
       " 'unwind': 2.995732273553991,\n",
       " 'gifted': 2.995732273553991,\n",
       " 'boss': 2.995732273553991,\n",
       " 'skrrt': 2.995732273553991,\n",
       " 'blue': 2.995732273553991,\n",
       " 'the': 0.16251892949777494,\n",
       " 'out': 0.3566749439387324,\n",
       " 'singing': 2.995732273553991,\n",
       " 'reside': 2.995732273553991,\n",
       " 'prayin': 2.995732273553991,\n",
       " 'casualty': 2.995732273553991,\n",
       " 'closer': 2.995732273553991,\n",
       " 'country': 2.995732273553991,\n",
       " 'genitals': 2.995732273553991,\n",
       " 'weaver': 2.995732273553991,\n",
       " 'bhasobhe': 2.995732273553991,\n",
       " 'means': 2.995732273553991,\n",
       " 'church': 2.995732273553991,\n",
       " 'his': 1.8971199848858813,\n",
       " 'acquitted': 2.995732273553991,\n",
       " 'crossin': 2.995732273553991,\n",
       " 'so': 0.7985076962177716,\n",
       " 'position': 2.995732273553991,\n",
       " 'mapped': 2.995732273553991,\n",
       " 'press': 2.995732273553991,\n",
       " 'pull': 2.995732273553991,\n",
       " 'monday': 2.995732273553991,\n",
       " 'fame': 2.995732273553991,\n",
       " 'judge': 2.995732273553991,\n",
       " 'whole': 2.995732273553991,\n",
       " 'glory': 2.995732273553991,\n",
       " 'work': 2.302585092994046,\n",
       " 'admission': 2.995732273553991,\n",
       " 'buddies': 2.995732273553991,\n",
       " 'pack': 2.995732273553991,\n",
       " 'without': 2.995732273553991,\n",
       " 'kung-fu': 2.995732273553991,\n",
       " 'knock': 2.995732273553991,\n",
       " 'until': 2.995732273553991,\n",
       " 'passengers': 2.995732273553991,\n",
       " 'forgotten': 2.995732273553991,\n",
       " 'pajamas': 2.995732273553991,\n",
       " 'administer': 2.995732273553991,\n",
       " 'desert': 2.995732273553991,\n",
       " 'blues': 2.995732273553991,\n",
       " 'bossed': 2.302585092994046,\n",
       " 'thanks': 2.995732273553991,\n",
       " 'god': 1.0498221244986776,\n",
       " 'souls': 2.995732273553991,\n",
       " 'story': 2.995732273553991,\n",
       " 'eat': 2.995732273553991,\n",
       " 'beam': 2.995732273553991,\n",
       " 'scared': 2.995732273553991,\n",
       " 'though': 2.302585092994046,\n",
       " 'message': 2.995732273553991,\n",
       " 'holding': 2.995732273553991,\n",
       " 'lime': 2.995732273553991,\n",
       " 'lick': 2.995732273553991,\n",
       " 'bags': 2.995732273553991,\n",
       " 'breathe': 2.995732273553991,\n",
       " 'breaks': 2.995732273553991,\n",
       " 'blah': 2.995732273553991,\n",
       " 'honky-tonk': 2.995732273553991,\n",
       " 'tried': 2.302585092994046,\n",
       " 'part': 1.8971199848858813,\n",
       " 'victim': 2.995732273553991,\n",
       " 'truth': 2.995732273553991,\n",
       " 'triggering': 2.995732273553991,\n",
       " 'fishing': 2.995732273553991,\n",
       " 'week': 2.995732273553991,\n",
       " 'city': 1.8971199848858813,\n",
       " 'lox': 2.995732273553991,\n",
       " 'lit': 2.995732273553991,\n",
       " 'sure': 2.302585092994046,\n",
       " 'hang': 2.302585092994046,\n",
       " 'sand': 2.995732273553991,\n",
       " 'atlanta': 2.995732273553991,\n",
       " 'kenny': 2.995732273553991,\n",
       " 'champaign-urbana': 2.995732273553991,\n",
       " 'always': 1.3862943611198906,\n",
       " 'breakfast': 2.995732273553991,\n",
       " 'sands': 2.995732273553991,\n",
       " 'dough': 2.995732273553991,\n",
       " 'days': 2.302585092994046,\n",
       " 'yes': 2.302585092994046,\n",
       " 'thing': 2.995732273553991,\n",
       " 'kathleen': 2.995732273553991,\n",
       " 'sneak': 2.995732273553991,\n",
       " 'bet': 2.302585092994046,\n",
       " 'foreman': 2.995732273553991,\n",
       " 'if': 0.9162907318741551,\n",
       " 'alabama': 2.995732273553991,\n",
       " 'operator': 2.995732273553991,\n",
       " 'never': 0.9162907318741551,\n",
       " 'wine': 2.995732273553991,\n",
       " 'going': 1.6094379124341003,\n",
       " 'onto': 2.995732273553991,\n",
       " 'dog': 2.302585092994046,\n",
       " 'thou': 2.995732273553991,\n",
       " 'voicing': 2.995732273553991,\n",
       " 'broads': 2.995732273553991,\n",
       " 'greater': 2.995732273553991,\n",
       " 'murder': 2.995732273553991,\n",
       " 'soulo': 2.995732273553991,\n",
       " 'c4': 2.995732273553991,\n",
       " 'raised': 2.995732273553991,\n",
       " 'low': 2.995732273553991,\n",
       " 'tears': 2.995732273553991,\n",
       " 'show': 1.2039728043259361,\n",
       " 'pride': 2.995732273553991,\n",
       " 'most': 2.302585092994046,\n",
       " 'motion': 2.995732273553991,\n",
       " 'cause': 0.6931471805599453,\n",
       " ':': 1.8971199848858813,\n",
       " 'liney': 2.995732273553991,\n",
       " 'wakanda': 2.995732273553991,\n",
       " 'siena': 2.995732273553991,\n",
       " '9': 2.302585092994046,\n",
       " 'chosen': 2.995732273553991,\n",
       " 'hands': 1.8971199848858813,\n",
       " 'course': 2.995732273553991,\n",
       " 'different': 2.995732273553991,\n",
       " 'air': 2.995732273553991,\n",
       " 'pimp': 2.995732273553991,\n",
       " 'changes': 2.302585092994046,\n",
       " 'safe': 2.302585092994046,\n",
       " 'trees': 2.302585092994046,\n",
       " 'adds': 2.995732273553991,\n",
       " 'subliminal': 2.995732273553991,\n",
       " 'staples': 2.995732273553991,\n",
       " 'window': 2.995732273553991,\n",
       " 'says': 2.302585092994046,\n",
       " 'poison': 2.995732273553991,\n",
       " 'haters': 2.995732273553991,\n",
       " 'havana': 2.995732273553991,\n",
       " 'travis': 2.995732273553991,\n",
       " 'gave': 2.302585092994046,\n",
       " 'heard': 2.302585092994046,\n",
       " 'felt': 2.995732273553991,\n",
       " 'sea': 2.995732273553991,\n",
       " 'anymore': 2.995732273553991,\n",
       " 'ore': 2.995732273553991,\n",
       " 'sleight': 2.995732273553991,\n",
       " 'repeat': 2.995732273553991,\n",
       " 'dealin': 2.995732273553991,\n",
       " 'devoted': 2.995732273553991,\n",
       " 'hol': 2.995732273553991,\n",
       " 'c': 2.995732273553991,\n",
       " 'black': 2.302585092994046,\n",
       " 'rims': 2.995732273553991,\n",
       " '500': 2.995732273553991,\n",
       " 'guyana': 2.995732273553991,\n",
       " 'alive': 2.302585092994046,\n",
       " 'frozen': 2.995732273553991,\n",
       " 'millipede': 2.995732273553991,\n",
       " 'size': 2.995732273553991,\n",
       " 'beside': 2.995732273553991,\n",
       " 'they': 0.43078291609245434,\n",
       " 'done': 2.995732273553991,\n",
       " 'door': 2.995732273553991,\n",
       " 'fire': 1.8971199848858813,\n",
       " 'hit': 1.6094379124341003,\n",
       " 'walked': 2.995732273553991,\n",
       " 'counting': 2.995732273553991,\n",
       " 'emergency': 2.995732273553991,\n",
       " 'corrupt': 2.995732273553991,\n",
       " 'dust': 2.995732273553991,\n",
       " 'traffic': 2.995732273553991,\n",
       " 'doin': 2.995732273553991,\n",
       " 'colours': 2.995732273553991,\n",
       " 'spirit': 2.995732273553991,\n",
       " 'clique': 2.995732273553991,\n",
       " 'walking': 2.995732273553991,\n",
       " 'brown': 2.995732273553991,\n",
       " 'worked': 2.995732273553991,\n",
       " 'free': 1.8971199848858813,\n",
       " 'background': 2.995732273553991,\n",
       " 'band': 2.995732273553991,\n",
       " 'dinner': 2.995732273553991,\n",
       " 'fall': 1.8971199848858813,\n",
       " 'relief': 2.995732273553991,\n",
       " 'pissed': 2.995732273553991,\n",
       " 'kendrick': 2.995732273553991,\n",
       " 'entitled': 2.995732273553991,\n",
       " 'huh': 2.995732273553991,\n",
       " 'signatures': 2.995732273553991,\n",
       " 'bullshit': 2.995732273553991,\n",
       " 'rolled': 2.302585092994046,\n",
       " 'leaf': 2.995732273553991,\n",
       " 'now': 1.6094379124341003,\n",
       " 'how': 0.5978370007556204,\n",
       " 'trick': 2.995732273553991,\n",
       " 'side': 2.995732273553991,\n",
       " 'take': 1.0498221244986776,\n",
       " 'loss': 2.995732273553991,\n",
       " 'confetti': 2.995732273553991,\n",
       " 'prayer': 2.995732273553991,\n",
       " 'his-': 2.995732273553991,\n",
       " 'things': 2.302585092994046,\n",
       " 'wan': 1.3862943611198906,\n",
       " 'die': 2.302585092994046,\n",
       " 'flood': 2.995732273553991,\n",
       " 'other': 2.995732273553991,\n",
       " 'pass': 2.302585092994046,\n",
       " 'screaming': 2.995732273553991,\n",
       " 'status': 2.995732273553991,\n",
       " 'motherfucker': 2.995732273553991,\n",
       " 'sleep': 2.302585092994046,\n",
       " 'love': 1.2039728043259361,\n",
       " 'back': 1.2039728043259361,\n",
       " 'known': 2.995732273553991,\n",
       " 'moral': 2.995732273553991,\n",
       " 'sinister': 2.995732273553991,\n",
       " '911': 2.302585092994046,\n",
       " 'dock': 2.995732273553991,\n",
       " 'comes': 2.302585092994046,\n",
       " 'emotion': 2.995732273553991,\n",
       " 'young': 1.8971199848858813,\n",
       " 'live': 1.8971199848858813,\n",
       " 'made': 1.8971199848858813,\n",
       " 'morgue': 2.995732273553991,\n",
       " 'radar': 2.995732273553991,\n",
       " 'luck': 2.995732273553991,\n",
       " 'same': 1.8971199848858813,\n",
       " 'light': 1.8971199848858813,\n",
       " 'moves': 2.995732273553991,\n",
       " 'seems': 2.995732273553991,\n",
       " 'intensive': 2.995732273553991,\n",
       " 'thinking': 2.995732273553991,\n",
       " 'tangle': 2.995732273553991,\n",
       " 'there': 0.5978370007556204,\n",
       " 'lungs': 2.995732273553991,\n",
       " 'kind': 2.995732273553991,\n",
       " 'signed': 2.995732273553991,\n",
       " 'sunshine': 2.995732273553991,\n",
       " 'turns': 2.995732273553991,\n",
       " 'hollow': 2.995732273553991,\n",
       " 'stay': 2.995732273553991,\n",
       " 'he': 1.3862943611198906,\n",
       " 'land': 2.302585092994046,\n",
       " 'told': 2.995732273553991,\n",
       " 'herby': 2.995732273553991,\n",
       " 'dead': 2.995732273553991,\n",
       " 'bleeding': 2.995732273553991,\n",
       " 'charter': 2.995732273553991,\n",
       " 'rightful': 2.995732273553991,\n",
       " 'friends': 1.6094379124341003,\n",
       " 'chantin': 2.995732273553991,\n",
       " 'could': 1.8971199848858813,\n",
       " 'patois': 2.995732273553991,\n",
       " 'panther': 2.995732273553991,\n",
       " 'end': 1.6094379124341003,\n",
       " 'best': 2.302585092994046,\n",
       " 'drawn': 2.995732273553991,\n",
       " 'covered': 2.995732273553991,\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "songs_dict = [ count_vectorize(tokenize(clean_song(doc))) for doc in documents ]\n",
    "\n",
    "test_dict = inverse_document_frequency(songs_dict)\n",
    "\n",
    "test_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing TF-IDF\n",
    "\n",
    "Now that we can compute both Term Frequency and Inverse Document Frequency, computing an overall TF-IDF value is simple! All we need to do is multiply the two values.  \n",
    "\n",
    "In the cell below, complete the `tf_idf()` function.  This function should take in a list of dictionaries, just as the `inverse_document_frequency()` function did.  This function return a new list of dictionaries, with each dictionary containing the tf-idf vectorized representation of a corresponding song document. \n",
    "\n",
    "**_NOTE:_** Each document should contain the full vocabulary of the entire combined corpus.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(list_of_dicts):\n",
    "    # Create empty dictionary containing full vocabulary of entire corpus, to be used for each song\n",
    "    tf_idf_dict = {}\n",
    "    # Create list to hold those dicts\n",
    "    tf_idf_list = []\n",
    "    \n",
    "    #Leverage idf function to get unique words\n",
    "    #idf = inverse_document_frequency(list_of_dicts)\n",
    "    #all_words_dict = {word:0 for key in idf.keys()}\n",
    "    \n",
    "    # Create tf-idf list of dictionaries, containing a dictionary that will be updated \n",
    "    # for each document\n",
    "    for song in list_of_dicts:\n",
    "        #get term-frequency dict for this song\n",
    "        song_tf = term_frequency(song)\n",
    "        #get idf for this song and calc tf-idf\n",
    "        for word in song:\n",
    "            tf_\n",
    "        for \n",
    "        \n",
    "    \n",
    "    # Now, compute tf and then use this to compute and set tf-idf values for each document\n",
    "\n",
    "    \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing All Documents\n",
    "\n",
    "Now that we've created all the necessary helper functions, we can load in all of our documents and run each through the vectorization pipeline we've just created.\n",
    "\n",
    "In the cell below, complete the `main` function.  This function should take in a list of file names (provided for you in the `filenames` list we created at the start), and then:\n",
    "\n",
    "1. Read in each document\n",
    "1. Tokenize each document\n",
    "1. Convert each document to a Bag of Words (dictionary representation)\n",
    "1. Return a list of dictionaries vectorized using tf-idf, where each dictionary is a vectorized representation of a document.  \n",
    "\n",
    "**_HINT:_** Remember that all files are stored in the `data/` directory.  Be sure to append this to the filename when reading in each file, otherwise the path won't be correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(filenames):\n",
    "    pass\n",
    "\n",
    "tf_idf_all_docs = None\n",
    "print(list(tf_idf_all_docs[0])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing our Vectorizations\n",
    "\n",
    "Now that we have a tf-idf representation each document, we can move on to the fun part--visualizing everything!\n",
    "\n",
    "Let's investigate how many dimensions our data currently has.  In the cell below, examine our dataset to figure out how many dimensions our dataset has. \n",
    "\n",
    "**_HINT_**: Remember that every word is it's own dimension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dims = None\n",
    "print(\"Number of Dimensions: {}\".format(num_dims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much too high-dimensional for us to visualize! In order to make it understandable to human eyes, we'll need to reduce dimensionality to 2 or 3 dimensions.  \n",
    "\n",
    "### Reducing Dimensionality\n",
    "\n",
    "To do this, we'll use a technique called **_t-SNE_** (short for _t-Stochastic Neighbors Embedding_).  This is too complex for us to code ourselves, so we'll make use of sklearn's implementation of it.  \n",
    "\n",
    "First, we need to pull the words out of the dictionaries stored in `tf_idf_all_docs` so that only the values remain, and store them in lists instead of dictionaries.  This is because the t-SNE object only works with Array-like objects, not dictionaries.  \n",
    "\n",
    "In the cell below, create a list of lists that contains a list representation of the values of each of the dictionaries stored in `tf_idf_all_docs`.  The same structure should remain--e.g. the first list should contain only the values that were in the 1st dictionary in `tf_idf_all_docs`, and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vals_list = []\n",
    "\n",
    "for i in tf_idf_all_docs:\n",
    "    tf_idf_vals_list.append(list(i.values()))\n",
    "    \n",
    "tf_idf_vals_list[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have only the values, we can use the `TSNE` object from `sklearn` to transform our data appropriately.  In the cell below, create a `TSNE` with `n_components=3` passed in as a parameter.  Then, use the created object's `fit_transform()` method to transform the data stored in `tf_idf_vals_list` into 3-dimensional data.  Then, inspect the newly transformed data to confirm that it has the correct dimensionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sne_object_3d = None\n",
    "transformed_data_3d = None\n",
    "transformed_data_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also want to check out how the visualization looks in 2d.  Repeat the process above, but this time, create a `TSNE` object with 2 components instead of 3.  Again, use `fit_transform()` to transform the data and store it in the variable below, and then inspect it to confirm the transformed data has only 2 dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sne_object_2d = None\n",
    "transformed_data_2d = None\n",
    "transformed_data_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize everything!  Run the cell below to a 3D visualization of the songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kendrick_3d = transformed_data_3d[:10]\n",
    "k3_x = [i[0] for i in kendrick_3d]\n",
    "k3_y = [i[1] for i in kendrick_3d]\n",
    "k3_z = [i[2] for i in kendrick_3d]\n",
    "\n",
    "garth_3d = transformed_data_3d[10:]\n",
    "g3_x = [i[0] for i in garth_3d]\n",
    "g3_y = [i[1] for i in garth_3d]\n",
    "g3_z = [i[2] for i in garth_3d]\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(k3_x, k3_y, k3_z, c='b', s=60, label='Kendrick')\n",
    "ax.scatter(g3_x, g3_y, g3_z, c='red', s=60, label='Garth')\n",
    "ax.view_init(30, 10)\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "kendrick_2d = transformed_data_2d[:10]\n",
    "k2_x = [i[0] for i in kendrick_2d]\n",
    "k2_y = [i[1] for i in kendrick_2d]\n",
    "\n",
    "garth_2d = transformed_data_2d[10:]\n",
    "g2_x = [i[0] for i in garth_2d]\n",
    "g2_y = [i[1] for i in garth_2d]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(222)\n",
    "ax.scatter(k2_x, k2_y, c='b', label='Kendrick')\n",
    "ax.scatter(g2_x, g2_y, c='red', label='Garth')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! Take a crack at interpreting these graphs by answering the following question below:\n",
    "\n",
    "What does each graph mean? Do you find one graph more informative than the other? Do you think that this method shows us discernable differences between Kendrick Lamar songs and Garth Brooks songs?  Use the graphs and your understanding of TF-IDF to support your answer.  \n",
    "\n",
    "Write your answer to this question below this line:\n",
    "________________________________________________________________________________________________________________________________\n",
    "\n",
    "Both graphs show a basic trend among the red and blue dots, although the 3-dimensional graph is more informative than the 2-dimensional graph.  We see a separation between the two artists because they both have words that they use, but the other artist does not.  The words in each song that are common to both are reduced very small numbers or to 0, because of the log operation in the IDF function.  This means that the elements of each song vector with the highest values will be the ones that have words that are unique to that specific document, or at least are rarely used in others.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, we learned how to: \n",
    "* Tokenize a corpus of words and identify the different choices to be made while parsing them\n",
    "* Use a Count Vectorization strategy to create a Bag of Words\n",
    "* Use TF-IDF Vectorization with multiple documents to identify words that are important/unique to certain documents\n",
    "* Visualize and compare vectorized text documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['song19.txt',\n",
       " 'song10.txt',\n",
       " 'song11.txt',\n",
       " 'song15.txt',\n",
       " 'song14.txt',\n",
       " 'song18.txt',\n",
       " 'song2.txt',\n",
       " 'song17.txt',\n",
       " 'song4.txt',\n",
       " 'song20.txt',\n",
       " 'song7.txt',\n",
       " 'song6.txt',\n",
       " 'song1.txt',\n",
       " 'song12.txt',\n",
       " 'song13.txt',\n",
       " 'song9.txt',\n",
       " 'song5.txt',\n",
       " 'song3.txt',\n",
       " 'song8.txt',\n",
       " 'song16.txt']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "[filename for filename in os.listdir('data/')\n",
    " if 'song' in filename]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digression:\n",
    "\n",
    "Let's do our own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "filenames = glob.glob('data/song*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/song19.txt',\n",
       " 'data/song10.txt',\n",
       " 'data/song11.txt',\n",
       " 'data/song15.txt',\n",
       " 'data/song14.txt',\n",
       " 'data/song18.txt',\n",
       " 'data/song2.txt',\n",
       " 'data/song17.txt',\n",
       " 'data/song4.txt',\n",
       " 'data/song20.txt',\n",
       " 'data/song7.txt',\n",
       " 'data/song6.txt',\n",
       " 'data/song1.txt',\n",
       " 'data/song12.txt',\n",
       " 'data/song13.txt',\n",
       " 'data/song9.txt',\n",
       " 'data/song5.txt',\n",
       " 'data/song3.txt',\n",
       " 'data/song8.txt',\n",
       " 'data/song16.txt']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for filename in filenames:\n",
    "    with open(filename) as f:\n",
    "        text = \"\"\n",
    "        for line in f:\n",
    "            if line.strip().startswith('['):\n",
    "                continue\n",
    "            text += line\n",
    "    data[filename] = text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = list(data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Wakanda\\nWelcome\\nBig shot, hol' up, wait, peanut butter insides (no)\\nOutside, cocaine white, body look like Gentiles (Gentiles)\\nEmotion, emotion, emotion, emotional\\nWhy you emotional? Why you emotional?\\nAh, bitch, you emotional, yeah\\nBig shot, big shot, (hol' on, hol' on), peanut butter insides (hol' on)\\nOutside, cocaine white, body look like Gentiles (Gentiles)\\nEmotion, emotion, emotion, emotional\\nWhy you emotional? Why you emotional?\\nAh, bitch, you emotional, yeah\\nServe that work for Kung-Fu Kenny\\nGot juice, got work, got weight, got plenty\\nGot them, got her, got more, got Benji (yeah)\\nTop off gettin' topped-off in the city\\nBig Top Dawg and I dance on 'em like Diddy\\nPop off and I pop back like Fiddy (yeah)\\nI hit the ceiling and forgot about the floor (yeah)\\nBrand so big, got my haters on the ropes (yeah)\\nThis be the wave, plus I live on the coast (yeah)\\nWhen I touch a bag, young nigga do the most (yeah)\\nMmm, woo, and I Wakanda flex\\nAnd you know what time it is (yeah)\\nBut don't know what grindin' is\\nBaby, I care, baby, I swear\\nOnly one real nigga here\\nOnly two real niggas here (yeah)\\nBig shot, hol' up, wait, peanut butter insides (no)\\nOutside, cocaine white, body look like Gentiles (Gentiles)\\nEmotion, emotion, emotion, emotional\\nWhy you emotional? Why you emotional?\\nAh, bitch, you emotional, yeah\\nBig shot, big shot, (hol' on, hol' on), peanut butter insides (hol' on)\\nOutside, cocaine white, body look like Gentiles (Gentiles)\\nEmotion, emotion, emotion, emotional\\nWhy you emotional? Why you emotional?\\nAh, bitch, you emotional, yeah\\nBig shots (yeah)\\nPurped up, syrup'd up, nigga, like Pimp C\\nFlight suit, 'bouta slide down space city\\nTop down, she down under like Iggy (yeah, it's lit)\\nLockjaw when the night-call too litty\\nKnockoff, get your rocks off, got plenty\\nGot rocks, got Js, got 'Ye, got liney (yeah)\\nI need my engine, she need extensions (yeah)\\nDon't be the center-attention, just play your position (straight up)\\nGot my life on the admission 'cause they been up fishing (yeah, yeah)\\nRunnin' through these racks like I'm Moses off the boo\\nHop out the trees, whippin' up four, gettin' lean\\nFoggy, can't breathe, fuck her right off a Phillipe (alright)\\nOut in the street (yeah), chill with the gang, make it sweet\\nBig shots, yeah (do-do-do-do)\\nForget your name like I'm Steve\\nBig shot, hol' up, wait, peanut butter insides (no)\\nOutside, cocaine white, body look like Gentiles (Gentiles)\\nEmotion, emotion, emotion, emotional\\nWhy you emotional? Why you emotional?\\nAh, bitch, you emotional, yeah\\nBig shot, big shot, (hol' on, hol' on), peanut butter insides (hol' on)\\nOutside, cocaine white, body look like Gentiles (Gentiles)\\nEmotion, emotion, emotion, emotional\\nWhy you emotional? Why you emotional?\\nAh, bitch, you emotional, yeah\\n\""
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = vec.transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.02207968, 0.02207968, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_data = vec.transform([\"Happy Birthday Mando\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1307 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
